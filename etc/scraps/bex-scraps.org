* TODO make the change to const NIDs (on a new branch)
I always like to show how to actually make a change, and this seems like a good one.

** TODO collect some more metrics
- number of steps
- count each kind of hash lookup, and whether it was found
- count calls to ITE::norm (can do this in dispatcher as it sends/receives the answers)
- or just analyze the wip table when the solution comes in to see what's still wip?

** TODO =NID::is_tbl=
- add a new bit for tables (or just use existing T) ?
- redefine =is_const= to specifically check equality for I/O

** TODO implement directly in AST for now.
- completely eliminate the work from nano test

** TODO render const nodes with braille
Mostly because it can hold a 5-variable truth table or a
5-variable anf expression. braille font
binary decision diagrams
bdd: https://jsfiddle.net/tangentstorm/bLbayo6c/

** TODO implement whenhi / whenlo
this should let it get down to I,O and work for BDD automatically.

** TODO how to handle for ANF?
- simple const-const is easy
- how to do const + true ANF?
- i think implementing =when_hi= and =when_lo= /might/ be sufficent.

* TODO test the change!
- compare the steps taken for each node
- how to handle for AST?

* TODO back to our number

want it to run faster than brute force
but still maintain the benefits of caching

next higher numbers: truth table size doubles with each new input bit

but that means the number of possible truth tables squares
wouldn't actually be hard to store a truth table that big.
2^32 bits = 500 MB uncompressed. (why? well 2^32 pointer gives you 4 gb,
but that's bytes, and we need bits, so divide by 8. 4g / 8 = 0.5g, or 500mb)
That's a pretty huge file, but it's not *that* huge.
Maybe it's possible to have multiple worker threads generate the input truth
table in linear ram from a BDD, and a stream processing thread to combine them.
BDD itself is a compression algorithm, but maybe other compression algorithms
could be used to unpack truth tables.

* TODO bex/bdd community?
link in the description to a forum
Remains an exercise in optimization.

https://www.reddit.com/r/bex_rs/new/

* TODO future directions(?)
# probably move this to a document on bex
- refactor and reuse BDDSWarm components for ANF, future VHL bases
- generalize the wip/distributed solver
- extend the raw truth table idea to arbitrary registers
  - process with streaming instructions or the gpu
  - convert to/from BDD for compression
- mixed representation for wip
  (meaning registers at the bottom, bdd up top)
- lazy solving of regions
  (solve truth table left to right to reach first answer faster)
- combine bottom-up and top-down solving
- dynamic sifting (variable permutations)
- new base formats
  - zdd
  - bic
  - cnf ? sat solver
  - aig ? https://en.wikipedia.org/wiki/And-inverter_graph
- gpu and fpga workers
- var sets for functions of n vars, no matter which n they are
  there might be 500 input variables, but only using 15.
- better AST
  - track topmost variable in NID even for AST
  - allow any number of arguments
  - full combinatory logic
  - operations on xints (nid arrays)
- import/export stored functions
- apply functions across base types

* ------------------------------------------

* TODO more example(s) from old repo?
* TODO novel parts about bex

- algebraic normal form
- shell

* tangents
** 2^32-1

x-1 = largest 32-bit unsigned integer
      "negative zero" in ones compliment

four bytes:
  more colors than on your computer screen
  brightest color on screen
  maximum number of ip addresses
  four gb of ram

** too small

little more than half the population of earth
  https://en.wikipedia.org/wiki/World_population

414 people on earth have more money than that.
   https://www.forbes.com/billionaires/
$196.29 billion USD bezos
  21.43 trillion USD (2019)

zimbabwe:
  https://en.wikipedia.org/wiki/Hyperinflation#Ten_most_severe_hyperinflations_in_world_history
  https://en.wikipedia.org/wiki/Zimbabwean_dollar

* --- thoughts from train
- sha256 as motivator? solving tools in general aren't up for the challenge
- move the future directions to a separate file
- how you can get involved
- nid was gently encouraged by rust (working with the grain of the language)
  - might say not memory safe, but it can be saved and copied

- anf: in addition, the anf base attempts to do the and and xor operations on the data in this form
- the idea is that when you're manipulating formulas, there's likely to be a lot of reuse from
  operations distributing over each other, and often, that can be captured near the top
  of the graph, without necessarily merging every leaf.


- when you're just talking about formulas, the variable order doesn't matter.
- i called this pattern a, but in a traditional bdd, you number from the top down
- but if you think about these as infinite patterns that appear in almost every expression,
- it makes sense for them to always have the same name"

- early on; emphasise canonical representations. bdd and anf are both cannonical. ast is not.

- explain the "combining functions efficiently" paradox: compression reduces a lot of steps because you can work at the top of the dag.
- one operation at the top might match 2^n operations at the bottom.
- but there is also overhead of fetching nodes from memory
- so it makes sense to balance the two
- I think how big the "registers" are might have to do with how much entropy you expect your function to have.
- the more regularity and structure, the more working near the top will save work
- the more random your data, the better it is to stream
- so it might make sense to let users configure this on each run.


* bdd swarm tangent

But when I implemented it, something kinda interesting
happened.

The plan was to chop up the work for constructing a BDD,
and divide it among the the threads - one thread per CPU
core.

So I figured if I had 6 CPU cores, it would run 6 times
as fast.

Well the little laptop I use on the train has only has two
cores, but what happened is that when I switched over to the
swarm, some of the steps that used to take 20 seconds
started taking 0.

So it was a completely non-linear speedup.

So what happened?

I'm actually not completely sure.

I suspect that


* bex internals: too much to talk about here, and not relevant for intro
** TODO =00:30= nid: optimizations nudged by rust

Each one of these things is what I call a NID.
Nid is short for "node identifier".

Internally, each of them is a 64-bit number,
broken down into fields.

At the moment, AST nodes don't have a top
variable associated with them, so they just
show up as numbers.

For BDD nodes, it shows you the top level
variable, and also whether or not the node
is inverted.

That's because if two nodes are exactly the
same except all the ones and zeros are swapped.
That's the invert or "not" operation, then
they share the same entry in the database,
and only the bit changes.

So that means if you have a node and you want
to invert the whole function...

: dup not

... then bex doesn't even need to load the graph
into memory. It just flips that one bit directly
in the NID.

** =00:00= ITE::norm
You might ask why use NIDs instead of pointers.

I actually started out using pointers.

The thing is, making a memory and thread safe
graph structure out of pointers is hard to get right,
and in rust you have to do it right, because unless you
wrap everything in an unsafe block rust won't let you
make mistakes.

So I tried just storing all my nodes in a vector, and
using node ids, and suddenly the code was a whole lot
simpler, and I was getting a lot more done.

But also, being able to pack metadata into a single
register means there are some operations you can do
directly on NIDs, without having to follow a pointer
at all.

So for example, if you run a profiler on bex, and you
pretty much anything with a BDD, you'll probably find
that almost all the time is actually spent in this
function called ITE::norm.

ITE means "if then else". It operates on three NIDs,
and it really needs to know whether the node is inverted,
and which input variable the node branches on.

Since that stuff is stored in the NID itself, this
function can do its work entirely in the CPU, without
reaching out to RAM at all.

** =00:30= wip: multi-core support

The examples we've tried so far only take a few
milliseconds to run, but in real life, if you want
to use this to lay out a circuit or as part of a
SAT solver, you have to deal with huge expressions
of hundreds or thousands of variables.

One of the things I was interested in with the rust
implementation was multi-threaded support.

# show https://www.rust-lang.org/

Rust's slogan is that it empowers everyone to build
reliant and efficient software.

And in particular, unless you explicitly opt out
of the checking system, it catches all sorts of
mistakes when it comes to thread and memory safety.

Over the years, I've always kind of avoided or
minimized multi-threading in my code just because
it really is so hard to get right, and I'm not
usually working on things where speed is all that
important.

But rust was promising to make multi-threading easy,
and I decided that I'd treat bex as sort of an
ongoing excercise in optimization.

So, I spent some time on it, and made a pretty
clunky multi-threaded worker for bex called the
BDDSwarm.

One of my current plans is to clean the swarm code up
and apply the same idea to some of the other graph
representations that bex supports, so maybe someday
I'll make another video to explain it.

This was my first serious attempt at a multi-threaded
system in my life, and the code for that part is way
too complicated and messy to talk about in this video.

What's important for this story, though, is that once
I got the multi-core stuff working, it started to look
like maybe bex could actually become a useful
application at some point.

And so I decided to come up with some standardized
benchmarks.


** =1:00= Inside the solver
*** TODO talk about xints
*** what's the point?

But you might ask, what's the point of this?

First of all, I already know the answer to the problem,
because that's what I started with.

Second of all, who cares?

The point isn't really to solve this particular problem.
The point is to solve whatever problem you throw at it
as quickly as possible.

The solution algorithm I'm using is pretty simplistic.

*** So what can we do?

Well one nice thing about the factoring problem is that
it scales way down.

A few versions of the problem actually run in a few seconds
on my machine.

: cargo test

Some of these are just general unit tests.

By the way, if you add one character to the j program then instead of
the final product, you'll see the running product, which is the first
15 primorials.

: */\p:i.15
: ,.*/\p:i.15

So currently, bex can solve the first four of these fast enough to
run as unit tests.

: cargo test --lib nano_bdd

#+begin_src rust
#[test] pub fn test_nano_bdd() {
  use {bdd::BDDBase, int::{X2,X4}};
  find_factors!(BDDBase, X2, X4, 6, vec![(2,3)], false); }
#+end_src

let's run again with that false changed to true.

*** TODO describe the diagrams that show up
eq.svg is the multiplication
lt.svg is the condition that x<y
ast.svg is the combination of those two
x-final.svg is the final AST

**** TODO show node numbers in the AST (before and after renumbering)
**** TODO render and show each step as a (stop-motion) "animation"

*** TODO generate diagrams with the original and reverse orders
use custom shapes https://www.graphviz.org/doc/info/shapes.html
now that #1 is at the bottom...

** slowtests and import/export

210 is an 8-bit number, and the tests look for two four-bit factors.
If I ask it to search for two 8-bit numbers that multiply to 210 as
a 16-bit number, then it winds up taking 11 minutes. Of course I don't
actually need all 16 bits in the answer, so it might be interesting
to have it discard the 16 bits in the AST stage.

(Which means it ought to also take 11 minutes for solving 30030)

But also, the way this works, it generates the entire BDD for
the multiplication of two input numbers from scratch in a fresh BDD
base every single time, even though this is completely generic.
There's no reason this function couldn't be cached to disk and
loaded into the base on demand.

Then it would just be a matter of pulling that pre-compiled function
in from a stored library.

Bex doesn't yet have an import feature at runtime, but you can save
and entire bases. Import and export should only be a few lines of code.
It's not hard at all, just something I haven't gotten around to.

*** TODO make and show a ticket for import/export

also there could be one stored multiplication database, 2*n output
bits for 2*n input bits, and you could just look at the ones you wanted.

import/export is easy, but i'd also have to teach the solver when to
use the imported function, which means having AST nodes aware of n-bit
ints... Which means making the AST representation much more expressive
in general.

(this is something i'm thinking about)


* TODO pascal git is missing some pieces
* cut anf example
: (1+a)(b+c)(a+b)                  // 6 terms (4 unique)
: (1+a)(b(a+b)+c(a+b))
: (1+a)(b(a+b)+ca+cb))
: (1+a)(ba+bb+ca+cb)
: (1+a)(ba+b+ca+cb)
: (ba+b+ca+cb)+a(ba+b+ca+cb)
: (ba+b+ca+cb)+ba+ba+ca+cba
: ba+b+ca+cb+ba+ba+ca+cba
: ab+b+ac+bc+ab+ab+ac+abc
: ab+ab+ab+abc+ac+ac+b+bc          // cancel
:       ab+abc      +b+bc          // 4 terms (4 unique)
: a(b+bc)+(b+bc)                                              b(a+ac+1+c)      // not allowed
: a(b(1+c)) + (b(1+c))                                        b(1+a+ac+c)
: a(b(1+c)) + b(1+c)                                          b(1+a(1+c)+c)



** xor fiddle

xor: https://jsfiddle.net/tangentstorm/vkmLq2bj/latest/
